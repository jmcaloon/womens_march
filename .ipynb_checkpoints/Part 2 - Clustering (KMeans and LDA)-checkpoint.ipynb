{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import LatentDirichletAllocation,TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employment &amp; Earnings</th>\n",
       "      <th>Political Participation</th>\n",
       "      <th>Poverty &amp; Opportunity</th>\n",
       "      <th>Reproductive Rights</th>\n",
       "      <th>Health &amp; Well-Being</th>\n",
       "      <th>Work &amp; Family</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Employment & Earnings  Political Participation  \\\n",
       "Name                                                         \n",
       "Alabama                        46                       41   \n",
       "Alaska                          7                       33   \n",
       "Arizona                        34                       14   \n",
       "Arkansas                       47                       47   \n",
       "California                     15                        8   \n",
       "\n",
       "            Poverty & Opportunity  Reproductive Rights  Health & Well-Being  \\\n",
       "Name                                                                          \n",
       "Alabama                        45                   40                   50   \n",
       "Alaska                         12                   29                   27   \n",
       "Arizona                        35                   24                   28   \n",
       "Arkansas                       50                   43                   47   \n",
       "California                     23                    9                   17   \n",
       "\n",
       "            Work & Family  \n",
       "Name                       \n",
       "Alabama                39  \n",
       "Alaska                 15  \n",
       "Arizona                38  \n",
       "Arkansas                8  \n",
       "California              2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_ranks_df = pd.read_csv(\"state_rankings.csv\")\n",
    "state_ranks_df.index = state_ranks_df.Name\n",
    "state_ranks_df.drop([\"Name\",\"Tweets Per Person\"], axis=1, inplace=True)\n",
    "state_ranks_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "\n",
    "def get_kmeans(data, k, scale=True):\n",
    "    if scale:\n",
    "        s = MinMaxScaler()\n",
    "        data = s.fit_transform(data)\n",
    "    \n",
    "    m = KMeans(n_clusters=k, random_state=0).fit(data)\n",
    "    d = m.predict(data)\n",
    "    return m, d        \n",
    "\n",
    "kmean_m, kmean_d = get_kmeans(state_ranks_df, n_topics, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employment &amp; Earnings</th>\n",
       "      <th>Political Participation</th>\n",
       "      <th>Poverty &amp; Opportunity</th>\n",
       "      <th>Reproductive Rights</th>\n",
       "      <th>Health &amp; Well-Being</th>\n",
       "      <th>Work &amp; Family</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Employment & Earnings  Political Participation  \\\n",
       "Name                                                         \n",
       "Alabama                        46                       41   \n",
       "Alaska                          7                       33   \n",
       "Arizona                        34                       14   \n",
       "Arkansas                       47                       47   \n",
       "California                     15                        8   \n",
       "\n",
       "            Poverty & Opportunity  Reproductive Rights  Health & Well-Being  \\\n",
       "Name                                                                          \n",
       "Alabama                        45                   40                   50   \n",
       "Alaska                         12                   29                   27   \n",
       "Arizona                        35                   24                   28   \n",
       "Arkansas                       50                   43                   47   \n",
       "California                     23                    9                   17   \n",
       "\n",
       "            Work & Family  Cluster  \n",
       "Name                                \n",
       "Alabama                39        2  \n",
       "Alaska                 15        0  \n",
       "Arizona                38        3  \n",
       "Arkansas                8        2  \n",
       "California              2        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_ranks_df['Cluster'] = kmean_m.labels_.tolist()\n",
    "state_ranks_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "texts = tweets['Text'].tolist() \n",
    "\n",
    "#Words to ignore\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(frozenset(['http','https', 'don', 'gl','www','twitter', \n",
    "                                                      'got','bit','women', 'woman', 'like', 'thank', 'instagram', 'fb', 'ly', \n",
    "                                                      'goo', 'status', 'atus', 'st', 'tatus','repost', 'did', 'sta', 'tus', 'youtu', \n",
    "                                                      'com', 'pic','statu', 'facebook', 'youtube', 'li', 'll', '01', '2017', \n",
    "                                                      'make', 'let', 'need', '31', 'rt', 'ln', 'html']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "time history senators 100 postcard heard send voices source twi\n",
      "Topic 1:\n",
      "stat day movement inauguration social start dems c0nvey theresistance supporters\n",
      "Topic 2:\n",
      "march womens signs post female join human important blog solidarity\n",
      "Topic 3:\n",
      "resist sciencemarch la ow yes action marching en stopsessions check\n",
      "Topic 4:\n",
      "just want feminist weekend hope amazing washingtonpost fighting los justice\n",
      "Topic 5:\n",
      "muslimban nobannowall theresistance resist blacklivesmatter muslimbanprotest imstillwithher heretostay strongertogether trump\n",
      "Topic 6:\n",
      "protests nomuslimban good mmflint pink lsarsour pussy country etsy work\n",
      "Topic 7:\n",
      "indivisible html great didn blm liberals sallyyates anti democrats nytimes\n",
      "Topic 8:\n",
      "sign support org womensrights ppl muslim moveon inauguration sorry petitions\n",
      "Topic 9:\n",
      "read going latest paper 11e6 edition_id days dumptrump 10 soros\n",
      "Topic 10:\n",
      "resistance womensmarchonwashington girl know resist broadway hey looks does doesn\n",
      "Topic 11:\n",
      "notmypresident maga whyimarch love marchforlife nodapl theresistance lovetrumpshate trump buff\n",
      "Topic 12:\n",
      "trump rights news cnn think ban donald politics change msnbc\n",
      "Topic 13:\n",
      "week equality man proud thanks say aclu refugees power laxprotest\n",
      "Topic 14:\n",
      "broemmel ref amazon resist bonfils sr utf8 keywords qid mike\n",
      "Topic 15:\n",
      "washington new madonna real look photo rebekahworsham president march uk\n",
      "Topic 16:\n",
      "protest america right trump stop vote nobannowall share usa ve\n",
      "Topic 17:\n",
      "people theresistance amjoy peopleoath dmvfollowers nastywomanvote fight gop dc thenightlyshow\n",
      "Topic 18:\n",
      "potus watch stoppresidentbannon marched trump video islam hillary old immigrationban\n",
      "Topic 19:\n",
      "realdonaldtrump today world sharia life pro law trumpban left marches\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "def LDA(documents, max_df=0.95, min_df=2, max_features=1000, n_topics=20, n_top_words=10):\n",
    "    '''\n",
    "     tf_vectorizer:\n",
    "       - Strips out “stop words”\n",
    "       - Filters out terms that occur in more than 95% of the docs (max_df=0.95)\n",
    "       - Filters out terms that occur in only one document (min_df=2).\n",
    "       - Selects the 1,000 most frequently occuring words in the corpus.\n",
    "       - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n",
    "         document length on the tf_vectorizer values. \n",
    "    '''\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words=stop_words)\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=20, learning_method='online', \\\n",
    "                                    learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    display_topics(lda, tf_feature_names, n_top_words)\n",
    "#     print (lda.transform(tf))\n",
    "    \n",
    "LDA(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer = RegexpTokenizer(\"[a-z']+\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(t) for t in tokens] \n",
    "\n",
    "def get_tf(data, use_idf, max_df=1.0, min_df=1, ngram_range=(1,1), max_features=100):\n",
    "    if use_idf:\n",
    "        m = TfidfVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words, ngram_range=ngram_range, tokenizer=tokenize)\n",
    "    else:\n",
    "        m = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, \\\n",
    "                            stop_words=stop_words, ngram_range=ngram_range, tokenizer=tokenize)\n",
    "    \n",
    "    d = m.fit_transform(data)\n",
    "    return m, d\n",
    "\n",
    "tf_m, tf_d = get_tf(tweets['Text'], use_idf=False, max_df=0.95, min_df=2, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "\n",
    "def get_lda(data, n_topics):\n",
    "    m = LatentDirichletAllocation(n_topics=n_topics, max_iter=20, learning_method='online', \\\n",
    "                                  learning_offset=50., random_state=0).fit(data)\n",
    "    d = m.transform(data)\n",
    "    return m, d\n",
    "\n",
    "lda_m, lda_d = get_lda(tf_d, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 stemmed words per topic in LDA model\n",
      "\n",
      "Topic #0:\n",
      "time, postcard, senat, voic, histori, s, heard, send, sourc, twi, tter, talk, check, anoth, perfect\n",
      "Topic #1:\n",
      "march, washington, world, sharia, k, movement, read, g, life, law, pro, buff, islam, z, uk\n",
      "Topic #2:\n",
      "w, vote, resist, weekend, help, impeachtrump, democrat, indivis, unit, hey, huffingtonpost, lead, trump, senschum, report\n",
      "Topic #3:\n",
      "know, hat, thing, use, pussi, you'r, tweet, pink, didn't, pussyhat, wear, soro, etsi, order, vagina\n",
      "Topic #4:\n",
      "just, watch, v, m, youtub, r, t, fight, trump, want, equal, man, video, million, start\n",
      "Topic #5:\n",
      "broemmel, love, sr, ref, resist, amazon, bonfil, utf, resisttrump, qid, keyword, mike, stori, true, men\n",
      "Topic #6:\n",
      "protest, sign, stat, nomuslimban, mani, inaugur, x, day, org, work, proud, ppl, petit, mmflint, moveon\n",
      "Topic #7:\n",
      "resist, nobannowal, muslimban, theresist, notmypresid, blacklivesmatt, don't, strongertogeth, s, imstillwithh, heretostay, aclu, alternativefact, liber, great\n",
      "Topic #8:\n",
      "blm, crowd, dem, lgbt, stoptrump, i'll, yeah, ff, indi, theresist, size, exact, asian, latina, push\n",
      "Topic #9:\n",
      "theresist, amjoy, gop, madonna, nodapl, peopleoath, dmvfollow, stopsess, nastywomanvot, lovetrumpsh, thenightlyshow, weris, s, session, lsarsour\n",
      "Topic #10:\n",
      "right, womensmarchonwashington, yes, q, becaus, human, live, elect, hillari, went, constitut, matter, abort, confirm, womensmarchlondon\n",
      "Topic #11:\n",
      "p, whi, bp, week, march, new, photo, dc, womensright, real, rebekahworsham, h, everi, mt, pjnet\n",
      "Topic #12:\n",
      "muslim, trump, i'm, stop, polit, think, n, white, chang, presid, tri, speak, organ, kill, come\n",
      "Topic #13:\n",
      "trump, realdonaldtrump, support, muslimban, potus, maga, stoppresidentbannon, resist, usa, marchforlif, immigr, refuge, particip, broadway, sallyy\n",
      "Topic #14:\n",
      "today, ow, femin, sciencemarch, way, write, trump, nation, moment, follow, someth, act, differ, womensrightsarehumanright, togeth\n",
      "Topic #15:\n",
      "s, u, muslimbanprotest, look, muslimban, o, join, year, refugeeswelcom, can't, femal, trumpban, congress, speaker, old\n",
      "Topic #16:\n",
      "america, stand, action, whyimarch, y, la, post, pleas, share, good, trump, s, en, j, l\n",
      "Topic #17:\n",
      "peopl, girl, trump, onli, donald, dumptrump, ani, turn, better, word, plan, continu, justic, end, anyon\n",
      "Topic #18:\n",
      "e, d, b, c, f, news, utm, id, paper, latest, power, cc, edit, social, medium\n",
      "Topic #19:\n",
      "feminist, american, say, cnn, hope, countri, ban, rape, msnbc, truth, fact, realli, amaz, care, import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_topics(model, feature_names, n_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "print(\"Top 15 stemmed words per topic in LDA model\\n\")\n",
    "show_topics(lda_m, tf_m.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
