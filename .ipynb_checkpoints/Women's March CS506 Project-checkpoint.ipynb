{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We made history at #womensmarch . Now it’s time to make our voices heard. First up: send a postcard to your Senators https://www. womensmarch.com/100?source=twi tter& …\n",
      "[1]\n",
      "['We', 'made', 'history', 'at', '#womensmarch', '.', 'Now', 'it’s', 'time', 'to', 'make', 'our', 'voices', 'heard.', 'First', 'up:', 'send', 'a', 'postcard', 'to', 'your', 'Senators', 'https://www.', 'womensmarch.com/100?source=twi', 'tter&', '…']\n",
      "We\n",
      "made\n",
      "history\n",
      "at\n",
      "#womensmarch\n",
      ".\n",
      "Now\n",
      "it’s\n",
      "time\n",
      "to\n",
      "make\n",
      "our\n",
      "voices\n",
      "heard.\n",
      "First\n",
      "up:\n",
      "send\n",
      "a\n",
      "postcard\n",
      "to\n",
      "your\n",
      "Senators\n",
      "https://www.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 15: ',' expected after '\"'\n",
      "Skipping line 28: ',' expected after '\"'\n",
      "Skipping line 63: ',' expected after '\"'\n",
      "Skipping line 84: ',' expected after '\"'\n",
      "Skipping line 89: ',' expected after '\"'\n",
      "Skipping line 112: ',' expected after '\"'\n",
      "Skipping line 117: ',' expected after '\"'\n",
      "Skipping line 140: ',' expected after '\"'\n",
      "Skipping line 149: ',' expected after '\"'\n",
      "Skipping line 156: ',' expected after '\"'\n",
      "Skipping line 213: ',' expected after '\"'\n",
      "Skipping line 262: ',' expected after '\"'\n",
      "Skipping line 290: ',' expected after '\"'\n",
      "Skipping line 294: ',' expected after '\"'\n",
      "Skipping line 303: ',' expected after '\"'\n",
      "Skipping line 332: ',' expected after '\"'\n",
      "Skipping line 348: ',' expected after '\"'\n",
      "Skipping line 358: ',' expected after '\"'\n",
      "Skipping line 362: ',' expected after '\"'\n",
      "Skipping line 373: ',' expected after '\"'\n",
      "Skipping line 374: ',' expected after '\"'\n",
      "Skipping line 393: ',' expected after '\"'\n",
      "Skipping line 493: ',' expected after '\"'\n",
      "Skipping line 502: ',' expected after '\"'\n",
      "Skipping line 530: ',' expected after '\"'\n",
      "Skipping line 573: ',' expected after '\"'\n",
      "Skipping line 576: ',' expected after '\"'\n",
      "Skipping line 580: ',' expected after '\"'\n",
      "Skipping line 584: ',' expected after '\"'\n",
      "Skipping line 597: ',' expected after '\"'\n",
      "Skipping line 607: ',' expected after '\"'\n",
      "Skipping line 608: ',' expected after '\"'\n",
      "Skipping line 609: ',' expected after '\"'\n",
      "Skipping line 674: ',' expected after '\"'\n",
      "Skipping line 683: ',' expected after '\"'\n",
      "Skipping line 725: ',' expected after '\"'\n",
      "Skipping line 764: ',' expected after '\"'\n",
      "Skipping line 789: ',' expected after '\"'\n",
      "Skipping line 799: ',' expected after '\"'\n",
      "Skipping line 811: ',' expected after '\"'\n",
      "Skipping line 841: ',' expected after '\"'\n",
      "Skipping line 852: ',' expected after '\"'\n",
      "Skipping line 914: ',' expected after '\"'\n",
      "Skipping line 928: ',' expected after '\"'\n",
      "Skipping line 939: ',' expected after '\"'\n",
      "Skipping line 967: ',' expected after '\"'\n",
      "Skipping line 970: ',' expected after '\"'\n",
      "Skipping line 972: ',' expected after '\"'\n",
      "Skipping line 981: ',' expected after '\"'\n",
      "Skipping line 998: ',' expected after '\"'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-24a42acd690c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"http\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"https\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwithout_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import got3 as got\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import text \n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "consumer_key = '3FJT3EvtOssiHYfw2xpH4Q9dR'\n",
    "consumer_secret = 'OzV47fYGizND0NJlaVYyg0ieoiwmCTHeCi9eMMAKVqCxC9QatT'\n",
    "\n",
    "access_token = '4879319992-CcCKl9DKwrsZIUItxOeUAA7GgfH5Md8XkyaKYx9'\n",
    "access_token_secret = 'Rva5W0Ttx1FDRBZW6rtUJT1GRQOjqehOWnFFBKElWrdD5'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False)\n",
    "\n",
    "texts = tweets['Text'].tolist() \n",
    "\n",
    "texts = texts[:2]\n",
    "print (texts[0])\n",
    "\n",
    "#Cut off url at end of tweet. Url has spaces in it so I had to do it this way\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    print ([1 for word in words if \"http\" in word])\n",
    "    print (words)\n",
    "    without_link = []\n",
    "    for word in words:\n",
    "        print (word)\n",
    "        if \"http\" in word:\n",
    "            print (hi)\n",
    "            break\n",
    "        without_link.append(word)\n",
    "        ' '.join(without_link)\n",
    "    text = without_link\n",
    "    print (text)\n",
    "\n",
    "    \n",
    "\n",
    "#texts = [word for word in text if text not in blacklist for text in texts ]\n",
    "\n",
    "# texts = []\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# # create English stop words list\n",
    "# en_stop = get_stop_words('en')\n",
    "\n",
    "# # Create p_stemmer of class PorterStemmer\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "# blacklist = [\"http\", \"twitter\", \"pic\", \"womensmarch\", \"com\", \"_\", \"s\", \"statu\"]\n",
    "    \n",
    "\n",
    "# for i in doc_set:\n",
    "#     raw = i.lower()\n",
    "#     tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "#     # remove stop words from tokens\n",
    "#     stopped_tokens = [i for i in tokens if not i in en_stop and not i in blacklist]\n",
    "    \n",
    "#     # stem tokens\n",
    "#     stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "#     # add tokens to list\n",
    "#     texts.append(stemmed_tokens)\n",
    "\n",
    "# # turn our tokenized documents into a id <-> term dictionary\n",
    "# dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# # convert tokenized documents into a document-term matrix\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# # generate LDA model\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "# print(ldamodel.print_topics(num_topics=2, num_words=4))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "\n",
    "\n",
    "def LDA(documents,max_features=100, max_df=0.5, no_topics=20, no_top_words=5):\n",
    "    \n",
    "    '''\n",
    "     tf_vectorizer:\n",
    "       - Strips out “stop words”\n",
    "       - Filters out terms that occur in more than 95% of the docs (max_df=0.95)\n",
    "       - Filters out terms that occur in only one document (min_df=2).\n",
    "       - Selects the 1,000 most frequently occuring words in the corpus.\n",
    "       - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n",
    "         document length on the tf_vectorizer values. \n",
    "    '''\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=2, max_features=1000, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=20, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "    display_topics(lda, tf_feature_names, no_top_words)\n",
    "    \n",
    "#LDA(texts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame as dataF\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://statusofwomendata.org/state-data/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "stateNames = []\n",
    "employmentEarn = []\n",
    "politicalPart = []\n",
    "povertyOpp = []\n",
    "reprodRights = []\n",
    "healthWell = []\n",
    "workFam = []\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "time.sleep(0.5)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "statesList = soup.find(\"article\", class_=\"post\").find_all(\"p\")\n",
    "\n",
    "for state in statesList:\n",
    "    statePage = requests.get(state.find(\"a\").get(\"href\"), headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    stateSoup = BeautifulSoup(statePage.text, \"lxml\")\n",
    "    name = stateSoup.find(\"div\", class_=\"state\").find(\"h1\").text\n",
    "    stateNames.append(name)\n",
    "    \n",
    "    reportCard = stateSoup.find(\"div\", class_=\"cardSummary\").find_all(\"tr\")[1:]\n",
    "    employmentEarn.append(reportCard[0].find_all(\"td\")[1].text)\n",
    "    if(reportCard[1].find_all(\"td\")[1].text == \"-\"):\n",
    "        politicalPart.append(\"0\")\n",
    "    else:\n",
    "        politicalPart.append(reportCard[1].find_all(\"td\")[1].text)\n",
    "    povertyOpp.append(reportCard[2].find_all(\"td\")[1].text)\n",
    "    reprodRights.append(reportCard[3].find_all(\"td\")[1].text)\n",
    "    healthWell.append(reportCard[4].find_all(\"td\")[1].text)\n",
    "    workFam.append(reportCard[5].find_all(\"td\")[1].text)\n",
    "        \n",
    "stateRankings = dataF({\"name\": stateNames, \"Employment & Earnings\": employmentEarn, \"Political Participation\": politicalPart, \"Poverty & Opportunity\": povertyOpp, \"Reproductive Rights\": reprodRights, \"Health & Well-Being\": healthWell, \"Work & Family\": workFam})\n",
    "stateRankings.to_csv(\"state_rankings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
