{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets from US states: 4937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "us_states= {\n",
    "    'Alabama': ['AL', 'Al'],\n",
    "    'Alaska': ['AK', 'Ak'],\n",
    "    'Arizona': ['AZ', 'Az'],\n",
    "    'Arkansas': ['AR', 'Ar'],\n",
    "    'California': ['CA', 'Ca'],\n",
    "    'Colorado': ['CO', 'Co'],\n",
    "    'Connecticut': ['CT', 'Ct'],\n",
    "    'Delaware': ['DE', 'De'],\n",
    "    'District Of Columbia': ['DC', 'D.C.','Washington, DC', 'Washington, D.C.', 'Washington DC',\n",
    "                            'Washington D.C.', 'WASH. D.C.'],\n",
    "    'Florida': ['FL', 'Fl'],\n",
    "    'Georgia': ['GA', 'Ga'],\n",
    "    'Hawaii': ['HI', 'Hi'],\n",
    "    'Idaho': ['ID', 'Id'],\n",
    "    'Illinois': ['IL', 'Il'],\n",
    "    'Indiana': ['IN'],\n",
    "    'Iowa': ['IA', 'Ia'],\n",
    "    'Kansas': ['KS', 'Ks'],\n",
    "    'Kentucky': ['KY', 'Ky'],\n",
    "    'Louisiana': ['LA', 'La'],\n",
    "    'Maine': ['ME', 'Me'],\n",
    "    'Maryland': ['MD', 'Md'],\n",
    "    'Massachusetts': ['MA', 'Ma'],\n",
    "    'Michigan': ['MI', 'Mi'],\n",
    "    'Minnesota': ['MN', 'Mn'],\n",
    "    'Mississippi': ['MS', 'Ms'],\n",
    "    'Missouri': ['MO', 'Mo'],\n",
    "    'Montana': ['MT', 'Mt'],\n",
    "    'Nebraska': ['NE', 'Ne'],\n",
    "    'Nevada': ['NV', 'Ne'],\n",
    "    'New Hampshire': ['NH', 'Nh'],\n",
    "    'New Jersey': ['NJ', 'Nj'],\n",
    "    'New Mexico': ['NM', 'Nm'],\n",
    "    'New York': ['NY', 'Ny'],\n",
    "    'North Carolina': ['NC', 'Nc'],\n",
    "    'North Dakota': ['ND', 'Nd'],\n",
    "    'Ohio': ['OH', 'Oh'],\n",
    "    'Oklahoma': ['OK', 'Ok'],\n",
    "    'Oregon': ['OR', 'Or'],\n",
    "    'Pennsylvania': ['PA', 'Pa'],\n",
    "    'Rhode Island': ['RI', 'Ri'],\n",
    "    'South Carolina': ['SC', 'Sc'],\n",
    "    'South Dakota': ['SD', 'Sd'],\n",
    "    'Tennessee': ['TN', 'Tn'],\n",
    "    'Texas': ['TX', 'Tx'],\n",
    "    'Utah': ['UT', 'Ut'],\n",
    "    'Vermont': ['VT', 'Vt'],\n",
    "    'Virginia': ['VA', 'Va'],\n",
    "    'Washington': ['WA', 'Wash.', 'Wash', 'Washington State', 'Wa'],\n",
    "    'West Virginia': ['WV', 'Wv'],\n",
    "    'Wisconsin': ['WI', 'Wi'],\n",
    "    'Wyoming': ['WY', 'Wy'],\n",
    "}\n",
    "\n",
    "locations = tweets['Geo'].tolist()\n",
    "\n",
    "#Number of tweets from each state\n",
    "state_counts = {state: 0 for state in us_states}\n",
    "\n",
    "cleaned_locations = []\n",
    "\n",
    "for idx, tweet in tweets.iterrows():\n",
    "    location= tweet['Geo']\n",
    "    if type(location) is str: #make sure its not nan\n",
    "        found_state = False\n",
    "        d = {'Text':tweet['Text'], 'Hashtags': tweet['Hashtags']}\n",
    "        for key, values in us_states.items():\n",
    "            if key.lower() in location.lower():\n",
    "#                 print (location)\n",
    "#                 print (key)\n",
    "#                 print ()\n",
    "                found_state = True\n",
    "                state_counts[key] += 1\n",
    "                d['Location'] = key\n",
    "                cleaned_locations.append(d)\n",
    "                break\n",
    "            else:\n",
    "                for value in values:\n",
    "                    if len(value) == 2:\n",
    "                        if value in location.replace(',','').split():\n",
    "#                             print (location)\n",
    "#                             print (value)\n",
    "#                             print ()\n",
    "                            found_state = True\n",
    "                            state_counts[key] +=1\n",
    "                            d['Location'] = key\n",
    "                            cleaned_locations.append(d)\n",
    "                            break\n",
    "                    elif value in location:\n",
    "#                         print (location)\n",
    "#                         print (value)\n",
    "#                         print ()\n",
    "                        found_state = True\n",
    "                        state_counts[key] += 1\n",
    "                        d['Location'] = key\n",
    "                        cleaned_locations.append(d)\n",
    "                        break\n",
    "                    elif len(value) > 2 and value.lower() in location.lower():\n",
    "#                         print (location)\n",
    "#                         print (value)\n",
    "#                         print ()\n",
    "                        found_state = True\n",
    "                        state_counts[key] +=1\n",
    "                        d['Location'] = key\n",
    "                        cleaned_locations.append(d)\n",
    "                        break\n",
    "            if found_state == True:\n",
    "                break\n",
    "                \n",
    "tweets = pd.DataFrame(cleaned_locations)\n",
    "\n",
    "state_counts = pd.DataFrame(list(state_counts.items()), columns=['Name', 'Tweet Count (Normalized for State Population)'])\n",
    "\n",
    "print(\"Number of tweets from US states:\", len(tweets.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame as dataF\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://statusofwomendata.org/state-data/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "stateNames = []\n",
    "employmentEarn = []\n",
    "politicalPart = []\n",
    "povertyOpp = []\n",
    "reprodRights = []\n",
    "healthWell = []\n",
    "workFam = []\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "time.sleep(0.5)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "statesList = soup.find(\"article\", class_=\"post\").find_all(\"p\")\n",
    "\n",
    "for state in statesList:\n",
    "    statePage = requests.get(state.find(\"a\").get(\"href\"), headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    stateSoup = BeautifulSoup(statePage.text, \"lxml\")\n",
    "    name = stateSoup.find(\"div\", class_=\"state\").find(\"h1\").text\n",
    "    stateNames.append(name)\n",
    "    \n",
    "    reportCard = stateSoup.find(\"div\", class_=\"cardSummary\").find_all(\"tr\")[1:]\n",
    "    employmentEarn.append(reportCard[0].find_all(\"td\")[1].text)\n",
    "    if(reportCard[1].find_all(\"td\")[1].text == \"-\"):\n",
    "        politicalPart.append(\"0\")\n",
    "    else:\n",
    "        politicalPart.append(reportCard[1].find_all(\"td\")[1].text)\n",
    "    povertyOpp.append(reportCard[2].find_all(\"td\")[1].text)\n",
    "    reprodRights.append(reportCard[3].find_all(\"td\")[1].text)\n",
    "    healthWell.append(reportCard[4].find_all(\"td\")[1].text)\n",
    "    workFam.append(reportCard[5].find_all(\"td\")[1].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Number of tweets per state (adjusted for population) to state_rankings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stateRankings = dataF({\"Name\": stateNames, \"Employment & Earnings\": employmentEarn, \"Political Participation\": politicalPart, \"Poverty & Opportunity\": povertyOpp, \"Reproductive Rights\": reprodRights, \"Health & Well-Being\": healthWell, \"Work & Family\": workFam})\n",
    "stateRankings = stateRankings[[\"Name\",\"Employment & Earnings\", \"Political Participation\", \"Poverty & Opportunity\",\"Reproductive Rights\", \"Health & Well-Being\", \"Work & Family\"]]\n",
    "\n",
    "stateRankings.loc[stateRankings['Name'] == 'District Of Columbia', 'Political Participation'] = 25\n",
    "\n",
    "stateRankings = stateRankings.merge(state_counts, on=\"Name\")\n",
    "stateRankings.to_csv(\"state_rankings.csv\", index=False)\n",
    "\n",
    "populations = \n",
    "{'Alabama': 4863300,\n",
    "    'Alaska': 741894,\n",
    "    'Arizona': 6931071,\n",
    "    'Arkansas': 2988248,\n",
    "    'California': 39506094,\n",
    "    'Colorado': 5632271,\n",
    "    'Connecticut': \t3568174,\n",
    "    'Delaware': 960054,\n",
    "    'District Of Columbia': 691963,\n",
    "    'Florida': 20979964,\n",
    "    'Georgia': 10421344,\n",
    "    'Hawaii': ['HI', 'Hi'],\n",
    "    'Idaho': ['ID', 'Id'],\n",
    "    'Illinois': ['IL', 'Il'],\n",
    "    'Indiana': ['IN'],\n",
    "    'Iowa': ['IA', 'Ia'],\n",
    "    'Kansas': ['KS', 'Ks'],\n",
    "    'Kentucky': ['KY', 'Ky'],\n",
    "    'Louisiana': ['LA', 'La'],\n",
    "    'Maine': ['ME', 'Me'],\n",
    "    'Maryland': ['MD', 'Md'],\n",
    "    'Massachusetts': ['MA', 'Ma'],\n",
    "    'Michigan': ['MI', 'Mi'],\n",
    "    'Minnesota': ['MN', 'Mn'],\n",
    "    'Mississippi': ['MS', 'Ms'],\n",
    "    'Missouri': ['MO', 'Mo'],\n",
    "    'Montana': ['MT', 'Mt'],\n",
    "    'Nebraska': ['NE', 'Ne'],\n",
    "    'Nevada': ['NV', 'Ne'],\n",
    "    'New Hampshire': ['NH', 'Nh'],\n",
    "    'New Jersey': ['NJ', 'Nj'],\n",
    "    'New Mexico': ['NM', 'Nm'],\n",
    "    'New York': ['NY', 'Ny'],\n",
    "    'North Carolina': ['NC', 'Nc'],\n",
    "    'North Dakota': ['ND', 'Nd'],\n",
    "    'Ohio': ['OH', 'Oh'],\n",
    "    'Oklahoma': ['OK', 'Ok'],\n",
    "    'Oregon': ['OR', 'Or'],\n",
    "    'Pennsylvania': ['PA', 'Pa'],\n",
    "    'Rhode Island': ['RI', 'Ri'],\n",
    "    'South Carolina': ['SC', 'Sc'],\n",
    "    'South Dakota': ['SD', 'Sd'],\n",
    "    'Tennessee': ['TN', 'Tn'],\n",
    "    'Texas': ['TX', 'Tx'],\n",
    "    'Utah': ['UT', 'Ut'],\n",
    "    'Vermont': ['VT', 'Vt'],\n",
    "    'Virginia': ['VA', 'Va'],\n",
    "    'Washington': ['WA', 'Wash.', 'Wash', 'Washington State', 'Wa'],\n",
    "    'West Virginia': ['WV', 'Wv'],\n",
    "    'Wisconsin': ['WI', 'Wi'],\n",
    "    'Wyoming': ['WY', 'Wy'],\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
