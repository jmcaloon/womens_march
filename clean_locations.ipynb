{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': 34, 'Alaska': 4, 'Arizona': 70, 'Arkansas': 54, 'California': 674, 'Colorado': 164, 'Connecticut': 91, 'Delaware': 13, 'District Of Columbia': 327, 'Florida': 347, 'Georgia': 100, 'Hawaii': 16, 'Idaho': 20, 'Illinois': 150, 'Indiana': 75, 'Iowa': 29, 'Kansas': 40, 'Kentucky': 30, 'Louisiana': 46, 'Maine': 16, 'Maryland': 80, 'Massachusetts': 152, 'Michigan': 91, 'Minnesota': 45, 'Mississippi': 11, 'Missouri': 67, 'Montana': 13, 'Nebraska': 8, 'Nevada': 44, 'New Hampshire': 22, 'New Jersey': 92, 'New Mexico': 34, 'New York': 668, 'North Carolina': 140, 'North Dakota': 4, 'Ohio': 124, 'Oklahoma': 18, 'Oregon': 104, 'Pennsylvania': 121, 'Rhode Island': 10, 'South Carolina': 31, 'South Dakota': 3, 'Tennessee': 53, 'Texas': 303, 'Utah': 16, 'Vermont': 26, 'Virginia': 179, 'Washington': 139, 'West Virginia': 1, 'Wisconsin': 35, 'Wyoming': 3}\n",
      "Number of tweets from US states: 4937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "us_states= {\n",
    "    'Alabama': ['AL', 'Al'],\n",
    "    'Alaska': ['AK', 'Ak'],\n",
    "    'Arizona': ['AZ', 'Az'],\n",
    "    'Arkansas': ['AR', 'Ar'],\n",
    "    'California': ['CA', 'Ca'],\n",
    "    'Colorado': ['CO', 'Co'],\n",
    "    'Connecticut': ['CT', 'Ct'],\n",
    "    'Delaware': ['DE', 'De'],\n",
    "    'District Of Columbia': ['DC', 'D.C.','Washington, DC', 'Washington, D.C.', 'Washington DC',\n",
    "                            'Washington D.C.', 'WASH. D.C.'],\n",
    "    'Florida': ['FL', 'Fl'],\n",
    "    'Georgia': ['GA', 'Ga'],\n",
    "    'Hawaii': ['HI', 'Hi'],\n",
    "    'Idaho': ['ID', 'Id'],\n",
    "    'Illinois': ['IL', 'Il'],\n",
    "    'Indiana': ['IN'],\n",
    "    'Iowa': ['IA', 'Ia'],\n",
    "    'Kansas': ['KS', 'Ks'],\n",
    "    'Kentucky': ['KY', 'Ky'],\n",
    "    'Louisiana': ['LA', 'La'],\n",
    "    'Maine': ['ME', 'Me'],\n",
    "    'Maryland': ['MD', 'Md'],\n",
    "    'Massachusetts': ['MA', 'Ma'],\n",
    "    'Michigan': ['MI', 'Mi'],\n",
    "    'Minnesota': ['MN', 'Mn'],\n",
    "    'Mississippi': ['MS', 'Ms'],\n",
    "    'Missouri': ['MO', 'Mo'],\n",
    "    'Montana': ['MT', 'Mt'],\n",
    "    'Nebraska': ['NE', 'Ne'],\n",
    "    'Nevada': ['NV', 'Ne'],\n",
    "    'New Hampshire': ['NH', 'Nh'],\n",
    "    'New Jersey': ['NJ', 'Nj'],\n",
    "    'New Mexico': ['NM', 'Nm'],\n",
    "    'New York': ['NY', 'Ny'],\n",
    "    'North Carolina': ['NC', 'Nc'],\n",
    "    'North Dakota': ['ND', 'Nd'],\n",
    "    'Ohio': ['OH', 'Oh'],\n",
    "    'Oklahoma': ['OK', 'Ok'],\n",
    "    'Oregon': ['OR', 'Or'],\n",
    "    'Pennsylvania': ['PA', 'Pa'],\n",
    "    'Rhode Island': ['RI', 'Ri'],\n",
    "    'South Carolina': ['SC', 'Sc'],\n",
    "    'South Dakota': ['SD', 'Sd'],\n",
    "    'Tennessee': ['TN', 'Tn'],\n",
    "    'Texas': ['TX', 'Tx'],\n",
    "    'Utah': ['UT', 'Ut'],\n",
    "    'Vermont': ['VT', 'Vt'],\n",
    "    'Virginia': ['VA', 'Va'],\n",
    "    'Washington': ['WA', 'Wash.', 'Wash', 'Washington State', 'Wa'],\n",
    "    'West Virginia': ['WV', 'Wv'],\n",
    "    'Wisconsin': ['WI', 'Wi'],\n",
    "    'Wyoming': ['WY', 'Wy'],\n",
    "}\n",
    "\n",
    "locations = tweets['Geo'].tolist()\n",
    "\n",
    "#Number of tweets from each state\n",
    "state_counts = {state: 0 for state in us_states}\n",
    "\n",
    "cleaned_locations = []\n",
    "\n",
    "for idx, tweet in tweets.iterrows():\n",
    "    location= tweet['Geo']\n",
    "    if type(location) is str: #make sure its not nan\n",
    "        found_state = False\n",
    "        d = {'Text':tweet['Text'], 'Hashtags': tweet['Hashtags']}\n",
    "        for key, values in us_states.items():\n",
    "            if key.lower() in location.lower():\n",
    "#                 print (location)\n",
    "#                 print (key)\n",
    "#                 print ()\n",
    "                found_state = True\n",
    "                state_counts[key] += 1\n",
    "                d['Location'] = key\n",
    "                cleaned_locations.append(d)\n",
    "                break\n",
    "            else:\n",
    "                for value in values:\n",
    "                    if len(value) == 2:\n",
    "                        if value in location.replace(',','').split():\n",
    "#                             print (location)\n",
    "#                             print (value)\n",
    "#                             print ()\n",
    "                            found_state = True\n",
    "                            state_counts[key] +=1\n",
    "                            d['Location'] = key\n",
    "                            cleaned_locations.append(d)\n",
    "                            break\n",
    "                    elif value in location:\n",
    "#                         print (location)\n",
    "#                         print (value)\n",
    "#                         print ()\n",
    "                        found_state = True\n",
    "                        state_counts[key] += 1\n",
    "                        d['Location'] = key\n",
    "                        cleaned_locations.append(d)\n",
    "                        break\n",
    "                    elif len(value) > 2 and value.lower() in location.lower():\n",
    "#                         print (location)\n",
    "#                         print (value)\n",
    "#                         print ()\n",
    "                        found_state = True\n",
    "                        state_counts[key] +=1\n",
    "                        d['Location'] = key\n",
    "                        cleaned_locations.append(d)\n",
    "                        break\n",
    "            if found_state == True:\n",
    "                break\n",
    "                \n",
    "tweets = pd.DataFrame(cleaned_locations)\n",
    "\n",
    "print (state_counts)\n",
    "\n",
    "\n",
    "print(\"Number of tweets from US states:\", len(tweets.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame as dataF\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://statusofwomendata.org/state-data/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "stateNames = []\n",
    "employmentEarn = []\n",
    "politicalPart = []\n",
    "povertyOpp = []\n",
    "reprodRights = []\n",
    "healthWell = []\n",
    "workFam = []\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "time.sleep(0.5)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "statesList = soup.find(\"article\", class_=\"post\").find_all(\"p\")\n",
    "\n",
    "for state in statesList:\n",
    "    statePage = requests.get(state.find(\"a\").get(\"href\"), headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    stateSoup = BeautifulSoup(statePage.text, \"lxml\")\n",
    "    name = stateSoup.find(\"div\", class_=\"state\").find(\"h1\").text\n",
    "    stateNames.append(name)\n",
    "    \n",
    "    reportCard = stateSoup.find(\"div\", class_=\"cardSummary\").find_all(\"tr\")[1:]\n",
    "    employmentEarn.append(reportCard[0].find_all(\"td\")[1].text)\n",
    "    if(reportCard[1].find_all(\"td\")[1].text == \"-\"):\n",
    "        politicalPart.append(\"0\")\n",
    "    else:\n",
    "        politicalPart.append(reportCard[1].find_all(\"td\")[1].text)\n",
    "    povertyOpp.append(reportCard[2].find_all(\"td\")[1].text)\n",
    "    reprodRights.append(reportCard[3].find_all(\"td\")[1].text)\n",
    "    healthWell.append(reportCard[4].find_all(\"td\")[1].text)\n",
    "    workFam.append(reportCard[5].find_all(\"td\")[1].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Number of tweets per state (adjusted for population) to state_rankings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "populations = {'Alabama': 4863300,\n",
    "    'Alaska': 741894,\n",
    "    'Arizona': 6931071,\n",
    "    'Arkansas': 2988248,\n",
    "    'California': 39250017,\n",
    "    'Colorado': 5540545,\n",
    "    'Connecticut': \t3576452,\n",
    "    'Delaware': 952065,\n",
    "    'District Of Columbia': 681170,\n",
    "    'Florida': 20612439,\n",
    "    'Georgia': 10310371,\n",
    "    'Hawaii': 1428557,\n",
    "    'Idaho': 1683140,\n",
    "    'Illinois': 12801539,\n",
    "    'Indiana': 6633053,\n",
    "    'Iowa': 3134693,\n",
    "    'Kansas': 2907289,\n",
    "    'Kentucky': 4436974,\n",
    "    'Louisiana': 4681666,\n",
    "    'Maine': 1331479,\n",
    "    'Maryland': 6016447,\n",
    "    'Massachusetts': 6811779,\n",
    "    'Michigan': 9928300,\n",
    "    'Minnesota': 5519952,\n",
    "    'Mississippi': 2988726,\n",
    "    'Missouri': 6093000,\n",
    "    'Montana': 1042520,\n",
    "    'Nebraska': 1907116,\n",
    "    'Nevada': 2940058,\n",
    "    'New Hampshire': 1334795,\n",
    "    'New Jersey': 8944469,\n",
    "    'New Mexico': 2081015,\n",
    "    'New York': 19745289,\n",
    "    'North Carolina': 10146788,\n",
    "    'North Dakota': 757952,\n",
    "    'Ohio': 11614373,\n",
    "    'Oklahoma': 3923561,\n",
    "    'Oregon': 4093465,\n",
    "    'Pennsylvania': 12784227,\n",
    "    'Rhode Island': 1056426,\n",
    "    'South Carolina': 4961119,\n",
    "    'South Dakota': 865454,\n",
    "    'Tennessee': 6651194,\n",
    "    'Texas': 27862596,\n",
    "    'Utah': 3051217,\n",
    "    'Vermont': 624594,\n",
    "    'Virginia': 8411808,\n",
    "    'Washington': 7288000,\n",
    "    'West Virginia': 1831102,\n",
    "    'Wisconsin': 5778708,\n",
    "    'Wyoming': 585501,\n",
    "}\n",
    "\n",
    "for state, count in state_counts.items():\n",
    "    population = populations[state]\n",
    "    state_counts[state] = (state_counts[state]/population)\n",
    "\n",
    "state_counts_df = pd.DataFrame(list(state_counts.items()), columns=['Name', 'Tweets Per Person'])\n",
    "\n",
    "stateRankings = dataF({\"Name\": stateNames, \"Employment & Earnings\": employmentEarn, \"Political Participation\": politicalPart, \"Poverty & Opportunity\": povertyOpp, \"Reproductive Rights\": reprodRights, \"Health & Well-Being\": healthWell, \"Work & Family\": workFam})\n",
    "stateRankings = stateRankings[[\"Name\",\"Employment & Earnings\", \"Political Participation\", \"Poverty & Opportunity\",\"Reproductive Rights\", \"Health & Well-Being\", \"Work & Family\"]]\n",
    "\n",
    "stateRankings.loc[stateRankings['Name'] == 'District Of Columbia', 'Political Participation'] = 25\n",
    "\n",
    "\n",
    "stateRankings = stateRankings.merge(state_counts_df, on=\"Name\")\n",
    "stateRankings.to_csv(\"state_rankings.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
