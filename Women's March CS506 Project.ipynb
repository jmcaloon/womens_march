{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "Prints top 20 clusters and their 5 keywords. Also prints the probability of each tweet belonging to each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "time history senators send heard\n",
      "Topic 1:\n",
      "trump scotus change tiffanyatrump un_women\n",
      "Topic 2:\n",
      "refugeeswelcome aren supposed leave nomuslimban\n",
      "Topic 3:\n",
      "new cover york devos aca\n",
      "Topic 4:\n",
      "2018 thehill devos tsunami thanks\n",
      "Topic 5:\n",
      "people maga love free marchforlife\n",
      "Topic 6:\n",
      "dumptrump democrats muslimban female fgm\n",
      "Topic 7:\n",
      "way speak days 10 sure\n",
      "Topic 8:\n",
      "whyimarch lovetrumpshate mniwiconi notmypresident donaldtrump\n",
      "Topic 9:\n",
      "movement stay zendaya wordpress icantkeepquiet\n",
      "Topic 10:\n",
      "rights just man good rape\n",
      "Topic 11:\n",
      "photo womensmarchlondon vagina black paper\n",
      "Topic 12:\n",
      "feminism news marching feminist resistance\n",
      "Topic 13:\n",
      "standuptotrump ow sharia law march\n",
      "Topic 14:\n",
      "resistance watch indivisible resist etsy\n",
      "Topic 15:\n",
      "stat inauguration day proud urbanebox\n",
      "Topic 16:\n",
      "resist theresistance muslimban nobannowall aclu\n",
      "Topic 17:\n",
      "right usa oh hollywood womensmarchonwashington\n",
      "Topic 18:\n",
      "stupid tcot trumptrain mr_pinko described\n",
      "Topic 19:\n",
      "march womens protest read washington\n",
      "[[ 0.92083333  0.00416667  0.00416667 ...,  0.00416667  0.00416667\n",
      "   0.00416667]\n",
      " [ 0.05        0.05        0.05       ...,  0.05        0.05        0.05      ]\n",
      " [ 0.01        0.01        0.01       ...,  0.01        0.01        0.01      ]\n",
      " ..., \n",
      " [ 0.00833333  0.00833333  0.00833333 ...,  0.00833333  0.00833333\n",
      "   0.00833333]\n",
      " [ 0.0125      0.0125      0.0125     ...,  0.0125      0.0125      0.7625    ]\n",
      " [ 0.00714286  0.00714286  0.00714286 ...,  0.00714286  0.00714286\n",
      "   0.00714286]]\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import got3 as got\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import text \n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "consumer_key = '3FJT3EvtOssiHYfw2xpH4Q9dR'\n",
    "consumer_secret = 'OzV47fYGizND0NJlaVYyg0ieoiwmCTHeCi9eMMAKVqCxC9QatT'\n",
    "\n",
    "access_token = '4879319992-CcCKl9DKwrsZIUItxOeUAA7GgfH5Md8XkyaKYx9'\n",
    "access_token_secret = 'Rva5W0Ttx1FDRBZW6rtUJT1GRQOjqehOWnFFBKElWrdD5'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "texts = tweets['Text'].tolist() \n",
    "\n",
    "\n",
    "#Words to ignore\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(frozenset(['http','https', 'don', 'gl','www','twitter', \n",
    "                                                      'got','bit','women', 'woman', 'like', 'thank', 'instagram', 'fb', 'ly', \n",
    "                                                      'goo', 'status', 'atus', 'st', 'tatus','repost', 'did', 'sta', 'tus', 'youtu', \n",
    "                                                      'com', 'pic','statu', 'facebook', 'youtube', 'li', 'll', '01', '2017', \n",
    "                                                      'make', 'let', 'need', '31', 'rt', 'ln']))\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "\n",
    "def LDA(documents,max_features=100, max_df=0.95, no_topics=20, no_top_words=5):\n",
    "    \n",
    "    '''\n",
    "     tf_vectorizer:\n",
    "       - Strips out “stop words”\n",
    "       - Filters out terms that occur in more than 95% of the docs (max_df=0.95)\n",
    "       - Filters out terms that occur in only one document (min_df=2).\n",
    "       - Selects the 1,000 most frequently occuring words in the corpus.\n",
    "       - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n",
    "         document length on the tf_vectorizer values. \n",
    "    '''\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=3, max_features=1000, stop_words=stop_words)\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=20, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    display_topics(lda, tf_feature_names, no_top_words)\n",
    "    print (lda.transform(tf))\n",
    "    \n",
    "LDA(texts)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of US States Found:  4888\n",
      "             Username              Date  \\\n",
      "0         HeidiAGould  2017-01-31 18:59   \n",
      "1            Gravit80  2017-01-31 18:59   \n",
      "2           ReneeB_75  2017-01-31 18:59   \n",
      "3       A_Big_Hell_NO  2017-01-31 18:59   \n",
      "4         twistertitr  2017-01-31 18:59   \n",
      "5            nmp_wear  2017-01-31 18:58   \n",
      "6           ReneeB_75  2017-01-31 18:58   \n",
      "7          RedDawn329  2017-01-31 18:58   \n",
      "8      BlueChalkMedia  2017-01-31 18:57   \n",
      "9     jauregui20L75H3  2017-01-31 18:57   \n",
      "10     dianeroznowski  2017-01-31 18:57   \n",
      "11      wedfavorstore  2017-01-31 18:57   \n",
      "12       LibrarianJak  2017-01-31 18:57   \n",
      "13          ReneeB_75  2017-01-31 18:57   \n",
      "14          Fred_Vega  2017-01-31 18:56   \n",
      "15        DickensRick  2017-01-31 18:56   \n",
      "16          WentRogue  2017-01-31 18:56   \n",
      "17        HeintjeC123  2017-01-31 18:55   \n",
      "18        Quark_Trek9  2017-01-31 18:55   \n",
      "19        nora_loehle  2017-01-31 18:55   \n",
      "20          dkipp1353  2017-01-31 18:55   \n",
      "21           DGSpeaks  2017-01-31 18:55   \n",
      "22            27kinds  2017-01-31 18:55   \n",
      "23        I3LACKIRISH  2017-01-31 18:54   \n",
      "24         tamraraven  2017-01-31 18:54   \n",
      "25         revigilant  2017-01-31 18:54   \n",
      "26           EWADJD_D  2017-01-31 18:54   \n",
      "27          Kind4Kids  2017-01-31 18:54   \n",
      "28       ursusmiratus  2017-01-31 18:53   \n",
      "29         SweeetSpot  2017-01-31 18:53   \n",
      "...               ...               ...   \n",
      "4858      HollandReid  2017-01-30 17:30   \n",
      "4859       Morgayne69  2017-01-30 17:29   \n",
      "4860   misscriss48212  2017-01-30 17:29   \n",
      "4861         Jairalee  2017-01-30 17:29   \n",
      "4862   newhorizons777  2017-01-30 17:29   \n",
      "4863  femininecollect  2017-01-30 17:29   \n",
      "4864        kaoconnor  2017-01-30 17:28   \n",
      "4865  ValRichardson17  2017-01-30 17:28   \n",
      "4866     BrandonB0629  2017-01-30 17:28   \n",
      "4867     brunnercaffi  2017-01-30 17:28   \n",
      "4868       astrid3121  2017-01-30 17:28   \n",
      "4869          Virino3  2017-01-30 17:28   \n",
      "4870      juliannimal  2017-01-30 17:28   \n",
      "4871   adrianakertzer  2017-01-30 17:27   \n",
      "4872     TheCodeWords  2017-01-30 17:27   \n",
      "4873       SnSWatkins  2017-01-30 17:27   \n",
      "4874      kamalfizazi  2017-01-30 17:25   \n",
      "4875   iamhelenhayden  2017-01-30 17:24   \n",
      "4876      RhymNReason  2017-01-30 17:24   \n",
      "4877         Junebug3  2017-01-30 17:24   \n",
      "4878     RubyReckless  2017-01-30 17:24   \n",
      "4879  SandyRollTide14  2017-01-30 17:24   \n",
      "4880         flan_sup  2017-01-30 17:24   \n",
      "4881  LisaSherwood101  2017-01-30 17:24   \n",
      "4882   charlottemakes  2017-01-30 17:22   \n",
      "4883          KurzLex  2017-01-30 17:22   \n",
      "4884   Sharkb8oohhaha  2017-01-30 17:22   \n",
      "4885      mollyluna11  2017-01-30 17:22   \n",
      "4886      AMJedmonton  2017-01-30 17:21   \n",
      "4887  SandyRollTide14  2017-01-30 17:21   \n",
      "\n",
      "                                                   Text  \\\n",
      "0     We made history at #womensmarch . Now it’s tim...   \n",
      "1     ROFL! #WomensMarch https:// twitter.com/Elaine...   \n",
      "2     #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "3     I love listening to white men whine like pussi...   \n",
      "4     I'll B Voting 2018 #TheResistance #WomensMarch...   \n",
      "5     Steve Bannon: Unelected, Unvetted, Uncomfirmed...   \n",
      "6     #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "7     @MaddowBlog wow! We might just reach the 4-5mi...   \n",
      "8     7 students captured #360video of the inaugarat...   \n",
      "9     @Laurenjauregui @womensmarch #WomensMarch #Why...   \n",
      "10    When you tell your friend who proposed to @jon...   \n",
      "11    The latest The WFS Daily! http:// paper.li/wed...   \n",
      "12    Writing and sending postcards to our senators ...   \n",
      "13    #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "14    Putin and/or Bannon's puppet #IgnorancePrevail...   \n",
      "15    #DemForce #theresistence #MuslimBan #WomensMar...   \n",
      "16    Welcome sight in my mailbox tonight: look at t...   \n",
      "17    Contribute to the world. Donate to @aclu . Don...   \n",
      "18    #theresistance #starfleet #notmypresident #imp...   \n",
      "19    We don't stop #standuptotrump for freedom&clim...   \n",
      "20    Probably got run over by the #Womensmarch or #...   \n",
      "21    RT NationalNOW: .Terryoneill talks #WomensMarc...   \n",
      "22    I sent mine in, now it's your turn. Why did yo...   \n",
      "23    #WomensMarch So besides the crazy ramblings of...   \n",
      "24    #WomensMarch #ScienceMarch @350 @NRDC @SierraC...   \n",
      "25    My wife's #NYC coterie in Washington D.C.'s #W...   \n",
      "26    @MSNBC #WomensMarch We have to really Protest ...   \n",
      "27    Marching with my middle schooler http://www. m...   \n",
      "28    Not sure... women's march or superhero convent...   \n",
      "29    #TheResistance #StrongerTogether #BankExit #Du...   \n",
      "...                                                 ...   \n",
      "4858  Looks like I need new shoes for all this march...   \n",
      "4859  #stopsessions #StopDeVos #resist #WomensMarch ...   \n",
      "4860     Great read #muslimban #WomensMarch #LGBTrights   \n",
      "4861  Dear women: Why are you committing suicide wit...   \n",
      "4862  RT RT RT When #WomensMarch co-chair (Palestini...   \n",
      "4863  Why I am a #Feminist (and other men should be ...   \n",
      "4864  Trump's emotional tailspin http:// wpo.st/NNeX...   \n",
      "4865  Liberal #protest machine kicks into high gear ...   \n",
      "4866  #MuslimBan #WomensMarch First tweet just to be...   \n",
      "4867  Diese A Capella-Hymne vom #WomensMarch ist ein...   \n",
      "4868  #AntiTrump #BOYCOTT #CorporateTerrorism #Sales...   \n",
      "4869  De aardbevingsellende in Groningen is echt om ...   \n",
      "4870  Read #SanctuaryCities #MuslimBan #TrumpBan #Re...   \n",
      "4871  @emilyslist I understand fundraising is import...   \n",
      "4872  @FLOTUS #trump #WomensMarch #TrumpBan #MuslimB...   \n",
      "4873  Can @womensmarch help? Ready for another one. ...   \n",
      "4874  Need to re tweet this sign from the #WomensMar...   \n",
      "4875  @biannagolodryga @sallykohn @CecileRichards La...   \n",
      "4876  if it's true that the border was closed to ppl...   \n",
      "4877  From Anti-CoatHanger Citizen, #WomensMarch #wo...   \n",
      "4878  Brilliant! I couldn't make it tonight, but I w...   \n",
      "4879  Skank ho Madonna should be arrested for her th...   \n",
      "4880  I keep forgetting to post on this app but here...   \n",
      "4881  You have until tomorrow 12AM to sign up #Affor...   \n",
      "4882  Inspired by #WomensMarch and other #Marches we...   \n",
      "4883  We made history at #womensmarch . Now it’s tim...   \n",
      "4884  Damn, maybe you should inform #WomensMarch sup...   \n",
      "4885  first one is her modelling jumper for ssc auct...   \n",
      "4886  RT humanityfirstuk: Our Women's Health program...   \n",
      "4887  #ProLife standing up for a cause with class & ...   \n",
      "\n",
      "                                 Geo  \\\n",
      "0                                NaN   \n",
      "1             Mississippi Gulf Coast   \n",
      "2                                DMV   \n",
      "3                         Denver, CO   \n",
      "4                           Arkansas   \n",
      "5                                NaN   \n",
      "6                                DMV   \n",
      "7                       Arizona, USA   \n",
      "8      Brooklyn, NY and Portland, OR   \n",
      "9                          The World   \n",
      "10                    Washington, DC   \n",
      "11                            Online   \n",
      "12                      Syracuse, NY   \n",
      "13                               DMV   \n",
      "14           Orange, California, USA   \n",
      "15                               NaN   \n",
      "16    Texas ex on the Midwest tundra   \n",
      "17                        The planet   \n",
      "18                     New York, USA   \n",
      "19                     Washington DC   \n",
      "20                     Virginia, USA   \n",
      "21              38.950145,-77.020466   \n",
      "22                   Los Angeles, CA   \n",
      "23                               NaN   \n",
      "24           The Big River, Missouri   \n",
      "25                               NaN   \n",
      "26                     United States   \n",
      "27                    Washington, DC   \n",
      "28                               NaN   \n",
      "29                     United States   \n",
      "...                              ...   \n",
      "4858                         Atlanta   \n",
      "4859                              WA   \n",
      "4860                             NaN   \n",
      "4861                   Polysopherica   \n",
      "4862                             NaN   \n",
      "4863                   United States   \n",
      "4864                      El Paso TX   \n",
      "4865             Highlands Ranch, CO   \n",
      "4866                             NaN   \n",
      "4867                       Dübendorf   \n",
      "4868            San Francisco, CA!!!   \n",
      "4869                             NaN   \n",
      "4870                Lib Central, USA   \n",
      "4871                        New York   \n",
      "4872                             NaN   \n",
      "4873                    Santa Fe, NM   \n",
      "4874                NYC & Casablanca   \n",
      "4875              New York - Toronto   \n",
      "4876                   New York, USA   \n",
      "4877                     Bee Cave TX   \n",
      "4878                             NaN   \n",
      "4879                  Tuscaloosa, AL   \n",
      "4880                   Baltimore, MD   \n",
      "4881                   Colorado, USA   \n",
      "4882                  Cardiff, Wales   \n",
      "4883                     Chicago, IL   \n",
      "4884                     Atlanta, GA   \n",
      "4885    Ilkley if you can afford it.   \n",
      "4886                        Edmonton   \n",
      "4887                  Tuscaloosa, AL   \n",
      "\n",
      "                                               Hashtags  \\\n",
      "0                                          #womensmarch   \n",
      "1                                          #WomensMarch   \n",
      "2     #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "3                                          #womensmarch   \n",
      "4     #TheResistance #WomensMarch #Dems #Women #GOPW...   \n",
      "5     #realDonaldTrump #Resist #NotMyPresident #Anti...   \n",
      "6     #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "7                                          #WomensMarch   \n",
      "8                                #360video #WomensMarch   \n",
      "9     #WomensMarch #WhyWeMarch #whyIMarch #NoDAPL #N...   \n",
      "10                                         #womensmarch   \n",
      "11                              #womensmarch #sponsored   \n",
      "12    #womensmarch #whyimarch #womensrightsarehumanr...   \n",
      "13    #ImWithHer #WeRise #WomensMarch #NastyWomanVot...   \n",
      "14    #IgnorancePrevails #PeopleYouBeenDuped #Presid...   \n",
      "15     #DemForce #theresistence #MuslimBan #WomensMarch   \n",
      "16                                         #WomensMarch   \n",
      "17    #resist #RESISTANCE #lovewins #WomensMarch #Re...   \n",
      "18    #theresistance #starfleet #notmypresident #imp...   \n",
      "19    #standuptotrump #WomensMarch #ClimateMarch #Pe...   \n",
      "20                     #Womensmarch #BlockSessionsmarch   \n",
      "21                                         #WomensMarch   \n",
      "22                                         #womensmarch   \n",
      "23                                         #WomensMarch   \n",
      "24                    #WomensMarch #ScienceMarch #YOUTH   \n",
      "25                                    #NYC #WomensMarch   \n",
      "26                                         #WomensMarch   \n",
      "27                                         #WomensMarch   \n",
      "28                                         #WomensMarch   \n",
      "29    #TheResistance #StrongerTogether #BankExit #Du...   \n",
      "...                                                 ...   \n",
      "4858    #lgbtq #NoBanNoWall #WomensMarch #humanitymarch   \n",
      "4859  #stopsessions #StopDeVos #resist #WomensMarch ...   \n",
      "4860                #muslimban #WomensMarch #LGBTrights   \n",
      "4861                         #WomensMarch #stopsessions   \n",
      "4862                         #WomensMarch #womensrights   \n",
      "4863  #Feminist #womensmarch #womensrights #MondayBlogs   \n",
      "4864                 #resist #womensmarch #ImpeachTrump   \n",
      "4865           #protest #refugeeban #DeVos #WomensMarch   \n",
      "4866                            #MuslimBan #WomensMarch   \n",
      "4867                                       #WomensMarch   \n",
      "4868  #AntiTrump #BOYCOTT #CorporateTerrorism #Sales...   \n",
      "4869                     #WomensMarch #jinek #VVD #PVDA   \n",
      "4870  #SanctuaryCities #MuslimBan #TrumpBan #ResistT...   \n",
      "4871                                       #WomensMarch   \n",
      "4872           #trump #WomensMarch #TrumpBan #MuslimBan   \n",
      "4873                     #WomensMarch #ImpeachmentMarch   \n",
      "4874  #WomensMarch #RESISTANCE #resist #resistoften ...   \n",
      "4875                                       #WomensMarch   \n",
      "4876                                #WomensMarch #Trump   \n",
      "4877   #WomensMarch #womensrights #TheResistance #potus   \n",
      "4878                            #WomensMarch #StandTall   \n",
      "4879                                       #WomensMarch   \n",
      "4880                                       #WomensMarch   \n",
      "4881  #AffordableCareAct #NoDAPL #womensmarch #Black...   \n",
      "4882  #WomensMarch #Marches #stitching #placards #ha...   \n",
      "4883                                       #womensmarch   \n",
      "4884                                       #WomensMarch   \n",
      "4885                                       #WomensMarch   \n",
      "4886                            #Guatemala #WomensMarch   \n",
      "4887                              #ProLife #WomensMarch   \n",
      "\n",
      "                                       Location  \n",
      "0     (32.741646000000003, -89.678696000000002)  \n",
      "1             (39.059810999999996, -105.311104)  \n",
      "2              (34.969704, -92.373122999999993)  \n",
      "3     (33.729759000000001, -111.43122099999999)  \n",
      "4                       (42.165726, -74.948051)  \n",
      "5               (38.897438, -77.02681700000001)  \n",
      "6                       (42.165726, -74.948051)  \n",
      "7     (36.116203000000006, -119.68156399999999)  \n",
      "8     (31.054486999999998, -97.563461000000004)  \n",
      "9     (42.165725999999999, -74.948051000000007)  \n",
      "10              (38.897438, -77.02681700000001)  \n",
      "11             (37.769337, -78.169968000000011)  \n",
      "12            (36.116203000000006, -119.681564)  \n",
      "13    (38.456084999999995, -92.288368000000006)  \n",
      "14              (38.897438, -77.02681700000001)  \n",
      "15              (35.630066, -79.80641899999999)  \n",
      "16              (38.897438, -77.02681700000001)  \n",
      "17             (44.572021, -122.07093799999998)  \n",
      "18              (38.897438, -77.02681700000001)  \n",
      "19    (38.456084999999995, -92.288368000000006)  \n",
      "20            (36.116203000000006, -119.681564)  \n",
      "21             (34.969704, -92.373122999999993)  \n",
      "22                      (31.169546, -91.867805)  \n",
      "23              (38.897438, -77.02681700000001)  \n",
      "24    (36.116203000000006, -119.68156399999999)  \n",
      "25    (42.165725999999999, -74.948051000000007)  \n",
      "26                     (47.400902, -121.490494)  \n",
      "27    (27.766278999999997, -81.686782999999991)  \n",
      "28              (38.897438, -77.02681700000001)  \n",
      "29    (40.388783000000004, -82.764915000000002)  \n",
      "...                                         ...  \n",
      "4858                    (44.268543, -89.616508)  \n",
      "4859  (36.116203000000006, -119.68156399999999)  \n",
      "4860           (31.054486999999998, -97.563461)  \n",
      "4861                    (40.349457, -88.986137)  \n",
      "4862          (39.059810999999996, -105.311104)  \n",
      "4863  (27.766278999999997, -81.686782999999991)  \n",
      "4864            (38.897438, -77.02681700000001)  \n",
      "4865            (38.897438, -77.02681700000001)  \n",
      "4866  (42.165725999999999, -74.948051000000007)  \n",
      "4867           (40.388783000000004, -82.764915)  \n",
      "4868          (36.116203000000006, -119.681564)  \n",
      "4869   (27.766278999999997, -81.68678299999999)  \n",
      "4870          (36.116203000000006, -119.681564)  \n",
      "4871           (40.388783000000004, -82.764915)  \n",
      "4872                   (47.400902, -121.490494)  \n",
      "4873           (37.769337, -78.169968000000011)  \n",
      "4874            (37.769337, -78.16996800000001)  \n",
      "4875  (36.116203000000006, -119.68156399999999)  \n",
      "4876  (43.326617999999996, -84.536094999999989)  \n",
      "4877                   (47.400902, -121.490494)  \n",
      "4878  (42.165725999999999, -74.948051000000007)  \n",
      "4879  (42.165725999999999, -74.948051000000007)  \n",
      "4880            (37.769337, -78.16996800000001)  \n",
      "4881                    (31.169546, -91.867805)  \n",
      "4882                   (33.729759, -111.431221)  \n",
      "4883            (37.769337, -78.16996800000001)  \n",
      "4884                    (45.694454, -93.900192)  \n",
      "4885  (42.230170999999999, -71.530106000000004)  \n",
      "4886          (36.116203000000006, -119.681564)  \n",
      "4887            (38.897438, -77.02681700000001)  \n",
      "\n",
      "[4888 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "state_coords = pd.read_csv(\"statecoordinates.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "#convert dataframe to dictionary\n",
    "state_coords = state_coords.set_index('State').T.to_dict()\n",
    "\n",
    "alternate_names = {\"Washington, DC\": {'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008},\n",
    "     \"Washington, D.C.\": {'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008},\n",
    "     \"Washington DC\":{'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008},\n",
    "     \"Washington D.C.\":{'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008},\n",
    "     \"WASH. D.C.\":{'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008},\n",
    "     \"Wash.\":{'Latitude': 47.400902000000002, 'Longitude': -121.490494}}\n",
    "\n",
    "alternate_names.update(state_coords)\n",
    "state_coords = alternate_names\n",
    "\n",
    "\n",
    "abbr = {\"AL\": {'Latitude': 32.806671000000001, 'Longitude': -86.79113000000001},\"AK\":{'Latitude': 61.370716000000002, 'Longitude': -152.40441899999999}, \n",
    " \"AZ\": {'Latitude': 33.729759000000001, 'Longitude': -111.43122099999999},\"AR\":{'Latitude': 34.969704, 'Longitude': -92.373122999999993}, \n",
    " \"CA\": {'Latitude': 36.116203000000006, 'Longitude': -119.68156399999999},\"CO\":{'Latitude': 39.059810999999996, 'Longitude': -105.311104}, \n",
    " \"CT\": {'Latitude': 41.597782000000002, 'Longitude': -72.755370999999997},\"DC\":{'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008}, \n",
    " \"DE\": {'Latitude': 39.318522999999999, 'Longitude': -75.507141000000004},\"FL\": {'Latitude': 27.766278999999997, 'Longitude': -81.686782999999991},\n",
    " \"GA\": {'Latitude': 33.040619, 'Longitude': -83.643073999999999},\"HI\": {'Latitude': 21.094317999999998, 'Longitude': -157.49833700000002}, \n",
    " \"ID\": {'Latitude': 39.849426000000001, 'Longitude': -86.258278000000004},\"IL\":{'Latitude': 40.349457000000001, 'Longitude': -88.986136999999999}, \n",
    " \"IN\": {'Latitude': 39.849426000000001, 'Longitude': -86.258278000000004},\"IA\":{'Latitude': 42.011538999999999, 'Longitude': -93.210526000000002},\n",
    " \"KS\": {'Latitude': 38.526600000000002, 'Longitude': -96.726485999999994},\"KY\":{'Latitude': 37.668140000000001, 'Longitude': -84.670067000000003},\n",
    " \"LA\": {'Latitude': 31.169546, 'Longitude': -91.867805000000004},\"ME\":{'Latitude': 44.693946999999994, 'Longitude': -69.381927000000005}, \n",
    " \"MD\": {'Latitude': 39.063946000000001, 'Longitude': -76.802101000000008},\"MA\":{'Latitude': 42.230170999999999, 'Longitude': -71.530106000000004},\n",
    " \"MI\": {'Latitude': 43.326617999999996, 'Longitude': -84.536094999999989},\"MN\":{'Latitude': 45.694454, 'Longitude': -93.900192000000004},\n",
    " \"MS\": {'Latitude': 32.741646000000003, 'Longitude': -89.678696000000002},\"MO\":{'Latitude': 38.456084999999995, 'Longitude': -92.288368000000006},\n",
    " \"MT\": {'Latitude': 46.921925000000002, 'Longitude': -110.454353},\"NE\":{'Latitude': 41.125370000000004, 'Longitude': -98.268081999999993},\n",
    " \"NV\": {'Latitude': 38.313515000000002, 'Longitude': -117.055374},\"NH\":{'Latitude': 43.452491999999999, 'Longitude': -71.563896},\n",
    " \"NJ\": {'Latitude': 40.298904, 'Longitude': -74.521011000000001},\"NM\":{'Latitude': 34.840515000000003, 'Longitude': -106.24848200000001},\n",
    " \"NY\": {'Latitude': 42.165725999999999, 'Longitude': -74.948051000000007},\"NC\":{'Latitude': 35.630065999999999, 'Longitude': -79.806418999999991},\n",
    " \"ND\": {'Latitude': 47.528911999999998, 'Longitude': -99.784012000000004},\"OH\":{'Latitude': 40.388783000000004, 'Longitude': -82.764915000000002},\n",
    " \"OK\": {'Latitude': 35.565342000000001, 'Longitude': -96.928916999999998},\"OR\":{'Latitude': 44.572020999999999, 'Longitude': -122.07093799999998},\n",
    " \"PA\": {'Latitude': 40.590752000000002, 'Longitude': -77.209755000000001},\"RI\":{'Latitude': 41.680892999999998, 'Longitude': -71.511780000000002},\n",
    " \"SC\": {'Latitude': 33.856891999999995, 'Longitude': -80.94500699999999},\"SD\":{'Latitude': 44.299782, 'Longitude': -99.438828000000001},\n",
    " \"TN\": {'Latitude': 35.747844999999998, 'Longitude': -86.692344999999989},\"TX\":{'Latitude': 31.054486999999998, 'Longitude': -97.563461000000004},\n",
    " \"UT\": {'Latitude': 40.150032000000003, 'Longitude': -111.86243400000001}, \"VT\":{'Latitude': 44.045876, 'Longitude': -72.710685999999995},\n",
    " \"VA\": {'Latitude': 37.769337, 'Longitude': -78.169968000000011},\"WA\":{'Latitude': 47.400902000000002, 'Longitude': -121.490494},\n",
    " \"WV\": {'Latitude': 38.491225999999997, 'Longitude': -80.954453000000001},\"WI\":{'Latitude': 44.268543000000001, 'Longitude': -89.616507999999996},\n",
    " \"WY\": {'Latitude': 42.755966000000001, 'Longitude': -107.30248999999999}}\n",
    "\n",
    "state_coords.update(abbr)\n",
    "state_names = list(state_coords.keys())\n",
    "\n",
    "\n",
    "locations = tweets['Geo'].tolist()\n",
    "\n",
    "cleaned_locations = []\n",
    "\n",
    "#Number of tweets from each coordinate\n",
    "coord_counts = {}\n",
    "\n",
    "for location in locations:\n",
    "    \n",
    "    found_state = False\n",
    "    if type(location) is str: #make sure its not nan\n",
    "        for state in state_names:\n",
    "            if len(state) == 2:\n",
    "                if state in location.replace(',','').split():\n",
    "                    coords = (state_coords[state]['Latitude'], state_coords[state]['Longitude'])\n",
    "                    found_state = True\n",
    "                    if coords in coord_counts:\n",
    "                        coord_counts[coords] += 1\n",
    "                    else:\n",
    "                        coord_counts[coords] = 1\n",
    "                    cleaned_locations.append(str(coords))\n",
    "                    break\n",
    "                    \n",
    "            elif state in location:\n",
    "                coords = (state_coords[state]['Latitude'], state_coords[state]['Longitude'])\n",
    "                found_state = True\n",
    "                if coords in coord_counts:\n",
    "                    coord_counts[coords] += 1\n",
    "                else:\n",
    "                    coord_counts[coords] = 1\n",
    "                cleaned_locations.append(str(coords))\n",
    "                break\n",
    "                    \n",
    "            elif len(state) > 2 and state.lower() in location.lower():\n",
    "                coords = (state_coords[state]['Latitude'], state_coords[state]['Longitude'])\n",
    "                found_state = True\n",
    "                if coords in coord_counts:\n",
    "                    coord_counts[coords] += 1\n",
    "                else:\n",
    "                    coord_counts[coords] = 1\n",
    "                cleaned_locations.append(str(coords))\n",
    "                break\n",
    "                \n",
    "    \n",
    "print (\"Number of US States Found: \",len([x for x in cleaned_locations if x != 'None']))\n",
    "    \n",
    "name_counts = {}\n",
    "\n",
    "# Get dict with {state: number of tweets}\n",
    "for tup in list(coord_counts.keys()):\n",
    "    for key in state_coords:\n",
    "        if tup == (state_coords[key]['Latitude'], state_coords[key]['Longitude']):\n",
    "            name_counts[key] = coord_counts[tup]\n",
    "            break\n",
    "            \n",
    "import math\n",
    "\n",
    "\n",
    "cleaned_locations = [x for x in cleaned_locations if x != 'None' and not None]\n",
    "                     \n",
    "df = pd.DataFrame({\"Location\": cleaned_locations})\n",
    "\n",
    "tweets = pd.concat([tweets, df], axis=1, join=\"inner\")\n",
    "\n",
    "print (tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "Clustering hashtags. Still in beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Location': (34.969704, -92.373122999999993), 'Hashtags': ['#werise', '#nastywomanvote', '#amjoy', '#peopleoath', '#dmvfollowers'], '#werise': True, '#nastywomanvote': True, '#amjoy': True, '#peopleoath': True, '#dmvfollowers': True}, {'Location': (38.897438, -77.02681700000001), 'Hashtags': ['#resist', '#notmypresident', '#muslimban'], '#resist': True, '#notmypresident': True, '#muslimban': True}, {'Location': (42.165726, -74.948051), 'Hashtags': ['#werise', '#nastywomanvote', '#amjoy', '#peopleoath', '#dmvfollowers'], '#werise': True, '#nastywomanvote': True, '#amjoy': True, '#peopleoath': True, '#dmvfollowers': True}, {'Location': (42.165725999999999, -74.948051000000007), 'Hashtags': ['#whyimarch', '#notmypresident', '#lovetrumpshate'], '#whyimarch': True, '#notmypresident': True, '#lovetrumpshate': True}, {'Location': (36.116203000000006, -119.681564), 'Hashtags': ['#whyimarch'], '#whyimarch': True}]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '#werise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-83577e9c3099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#make vectors with location and hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_with_top_hashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jessicamcaloon/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0malways\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jessicamcaloon/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    177\u001b[0m                         \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                         \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '#werise'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tweets = tweets[['Location', 'Hashtags']]\n",
    "\n",
    "\n",
    "hashtags = tweets['Hashtags'].tolist()\n",
    "\n",
    "# ignore hashtags that are redundant or don't particularly mean anything\n",
    "blacklist = [\"#womensmarch\", \"#\", \"#theresistance\", \"#theresistence\", \"#resistance\", \"#women\"]\n",
    "new_hashtags = []\n",
    "for string in hashtags:\n",
    "    if type(string) is str:\n",
    "        string = string.lower()\n",
    "        lst = string.split()\n",
    "        for h in lst:\n",
    "            if h not in blacklist:\n",
    "                new_hashtags.append(h)\n",
    "hashtags = new_hashtags\n",
    "\n",
    "hashtags_ct = Counter(hashtags)\n",
    "top_hashtags= [x[0] for x in hashtags_ct.most_common(20)]\n",
    "#print (top_hashtags)\n",
    "\n",
    "tweets_dict = tweets.to_dict('records')\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "\n",
    "# only keep tweets with top hashtags\n",
    "# create dictionary with format {tweet id: {location, [top_hashtags]} }\n",
    "tweets_with_top_hashtags = []\n",
    "\n",
    "for t in tweets_dict:\n",
    "    if t['Location'] != 'None':\n",
    "        t['Hashtags'] = t['Hashtags'].lower().split()\n",
    "        t['Hashtags'] = [h for h in t['Hashtags'] if h in top_hashtags]\n",
    "        if len (t['Hashtags']) > 0:\n",
    "            tweets_with_top_hashtags.append(t)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "#prepare for vectorization\n",
    "for t in tweets_with_top_hashtags:\n",
    "    for h in t['Hashtags']:\n",
    "        t[h] = True\n",
    "        \n",
    "        \n",
    "print (take(5, tweets_with_top_hashtags))\n",
    "\n",
    "\n",
    "#make vectors with location and hashtags\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(tweets_with_top_hashtags).toarray()\n",
    "\n",
    "\n",
    "#if we use longitude and latitude, we would scale them here\n",
    "\n",
    "#Evaluate number clusters for kmeans via silhouette score\n",
    "max_clusters = 20\n",
    "\n",
    "def sc_evaluate_clusters(X,max_clusters):\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    s[0] = 0;\n",
    "    s[1] = 0;\n",
    "    for k in range(2,max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        s[k] = metrics.silhouette_score(X,kmeans.labels_,metric='euclidean')\n",
    "    plt.plot(range(2,len(s)),s[2:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(\"K-means silhouette scores\")\n",
    "    plt.show()\n",
    "    \n",
    "sc_evaluate_clusters(X,max_clusters)\n",
    "\n",
    "k=15\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "kmeans.fit_predict(X)\n",
    "k_time = time.time() - start_time\n",
    "\n",
    "# kmeans top terms per cluster \n",
    "asc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\n",
    "order_centroids = asc_order_centroids[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#cluster label is most popular term in each cluster\n",
    "print(\"Kmeans top term per cluster:\")\n",
    "k_clusters = set()\n",
    "for i in range(k):\n",
    "    for ind in order_centroids[i, :2]:\n",
    "        k_clusters.add(terms[ind])\n",
    "print(k_clusters)\n",
    "print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "With the data from https://statusofwomendata.org, we hope to determine if there's any correlation between a state's ranking in different women's issues and the number of #WomensMarch tweets were sent from that state. \n",
    "\n",
    "The outputted data can be found in state_rankings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame as dataF\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://statusofwomendata.org/state-data/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "stateNames = []\n",
    "employmentEarn = []\n",
    "politicalPart = []\n",
    "povertyOpp = []\n",
    "reprodRights = []\n",
    "healthWell = []\n",
    "workFam = []\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "time.sleep(0.5)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "statesList = soup.find(\"article\", class_=\"post\").find_all(\"p\")\n",
    "\n",
    "for state in statesList:\n",
    "    statePage = requests.get(state.find(\"a\").get(\"href\"), headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    stateSoup = BeautifulSoup(statePage.text, \"lxml\")\n",
    "    name = stateSoup.find(\"div\", class_=\"state\").find(\"h1\").text\n",
    "    stateNames.append(name)\n",
    "    \n",
    "    reportCard = stateSoup.find(\"div\", class_=\"cardSummary\").find_all(\"tr\")[1:]\n",
    "    employmentEarn.append(reportCard[0].find_all(\"td\")[1].text)\n",
    "    if(reportCard[1].find_all(\"td\")[1].text == \"-\"):\n",
    "        politicalPart.append(\"0\")\n",
    "    else:\n",
    "        politicalPart.append(reportCard[1].find_all(\"td\")[1].text)\n",
    "    povertyOpp.append(reportCard[2].find_all(\"td\")[1].text)\n",
    "    reprodRights.append(reportCard[3].find_all(\"td\")[1].text)\n",
    "    healthWell.append(reportCard[4].find_all(\"td\")[1].text)\n",
    "    workFam.append(reportCard[5].find_all(\"td\")[1].text)\n",
    "        \n",
    "stateRankings = dataF({\"name\": stateNames, \"Employment & Earnings\": employmentEarn, \"Political Participation\": politicalPart, \"Poverty & Opportunity\": povertyOpp, \"Reproductive Rights\": reprodRights, \"Health & Well-Being\": healthWell, \"Work & Family\": workFam})\n",
    "stateRankings.to_csv(\"state_rankings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
