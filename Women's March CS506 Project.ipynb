{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "Prints top 20 clusters and their 5 keywords. Also prints the probability of each tweet belonging to each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "time history senators send heard\n",
      "Topic 1:\n",
      "trump scotus change tiffanyatrump un_women\n",
      "Topic 2:\n",
      "refugeeswelcome aren supposed leave nomuslimban\n",
      "Topic 3:\n",
      "new cover york devos aca\n",
      "Topic 4:\n",
      "2018 thehill devos tsunami thanks\n",
      "Topic 5:\n",
      "people maga love free marchforlife\n",
      "Topic 6:\n",
      "dumptrump democrats muslimban female fgm\n",
      "Topic 7:\n",
      "way speak days 10 sure\n",
      "Topic 8:\n",
      "whyimarch lovetrumpshate mniwiconi notmypresident donaldtrump\n",
      "Topic 9:\n",
      "movement stay zendaya wordpress icantkeepquiet\n",
      "Topic 10:\n",
      "rights just man good rape\n",
      "Topic 11:\n",
      "photo womensmarchlondon vagina black paper\n",
      "Topic 12:\n",
      "feminism news marching feminist resistance\n",
      "Topic 13:\n",
      "standuptotrump ow sharia law march\n",
      "Topic 14:\n",
      "resistance watch indivisible resist etsy\n",
      "Topic 15:\n",
      "stat inauguration day proud urbanebox\n",
      "Topic 16:\n",
      "resist theresistance muslimban nobannowall aclu\n",
      "Topic 17:\n",
      "right usa oh hollywood womensmarchonwashington\n",
      "Topic 18:\n",
      "stupid tcot trumptrain mr_pinko described\n",
      "Topic 19:\n",
      "march womens protest read washington\n",
      "[[ 0.92083333  0.00416667  0.00416667 ...,  0.00416667  0.00416667\n",
      "   0.00416667]\n",
      " [ 0.05        0.05        0.05       ...,  0.05        0.05        0.05      ]\n",
      " [ 0.01        0.01        0.01       ...,  0.01        0.01        0.01      ]\n",
      " ..., \n",
      " [ 0.00833333  0.00833333  0.00833333 ...,  0.00833333  0.00833333\n",
      "   0.00833333]\n",
      " [ 0.0125      0.0125      0.0125     ...,  0.0125      0.0125      0.7625    ]\n",
      " [ 0.00714286  0.00714286  0.00714286 ...,  0.00714286  0.00714286\n",
      "   0.00714286]]\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import got3 as got\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import text \n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "consumer_key = '3FJT3EvtOssiHYfw2xpH4Q9dR'\n",
    "consumer_secret = 'OzV47fYGizND0NJlaVYyg0ieoiwmCTHeCi9eMMAKVqCxC9QatT'\n",
    "\n",
    "access_token = '4879319992-CcCKl9DKwrsZIUItxOeUAA7GgfH5Md8XkyaKYx9'\n",
    "access_token_secret = 'Rva5W0Ttx1FDRBZW6rtUJT1GRQOjqehOWnFFBKElWrdD5'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "texts = tweets['Text'].tolist() \n",
    "\n",
    "\n",
    "#Words to ignore\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(frozenset(['http','https', 'don', 'gl','www','twitter', \n",
    "                                                      'got','bit','women', 'woman', 'like', 'thank', 'instagram', 'fb', 'ly', \n",
    "                                                      'goo', 'status', 'atus', 'st', 'tatus','repost', 'did', 'sta', 'tus', 'youtu', \n",
    "                                                      'com', 'pic','statu', 'facebook', 'youtube', 'li', 'll', '01', '2017', \n",
    "                                                      'make', 'let', 'need', '31', 'rt', 'ln']))\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "\n",
    "def LDA(documents,max_features=100, max_df=0.95, no_topics=20, no_top_words=5):\n",
    "    \n",
    "    '''\n",
    "     tf_vectorizer:\n",
    "       - Strips out “stop words”\n",
    "       - Filters out terms that occur in more than 95% of the docs (max_df=0.95)\n",
    "       - Filters out terms that occur in only one document (min_df=2).\n",
    "       - Selects the 1,000 most frequently occuring words in the corpus.\n",
    "       - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n",
    "         document length on the tf_vectorizer values. \n",
    "    '''\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=3, max_features=1000, stop_words=stop_words)\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=20, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    display_topics(lda, tf_feature_names, no_top_words)\n",
    "    print (lda.transform(tf))\n",
    "    \n",
    "LDA(texts)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tweets = pd.read_csv(\"output_got.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "state_coords = pd.read_csv(\"statecoordinates.csv\", sep=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "#lower = lambda x: x.lower()\n",
    "state_coords['State'] = state_coords['State']\n",
    "\n",
    "#convert dataframe to dictionary\n",
    "state_coords = state_coords.set_index('State').T.to_dict()\n",
    "abbr = {\"AL\": {'Latitude': 32.806671000000001, 'Longitude': -86.79113000000001},\"AK\":{'Latitude': 61.370716000000002, 'Longitude': -152.40441899999999}, \n",
    " \"AZ\": {'Latitude': 33.729759000000001, 'Longitude': -111.43122099999999},\"AR\":{'Latitude': 34.969704, 'Longitude': -92.373122999999993}, \n",
    " \"CA\": {'Latitude': 36.116203000000006, 'Longitude': -119.68156399999999},\"CO\":{'Latitude': 39.059810999999996, 'Longitude': -105.311104}, \n",
    " \"CT\": {'Latitude': 41.597782000000002, 'Longitude': -72.755370999999997},\"DC\":{'Latitude': 38.897438000000001, 'Longitude': -77.026817000000008}, \n",
    " \"DE\": {'Latitude': 39.318522999999999, 'Longitude': -75.507141000000004},\"FL\": {'Latitude': 27.766278999999997, 'Longitude': -81.686782999999991},\n",
    " \"GA\": {'Latitude': 33.040619, 'Longitude': -83.643073999999999},\"HI\": {'Latitude': 21.094317999999998, 'Longitude': -157.49833700000002}, \n",
    " \"ID\": {'Latitude': 39.849426000000001, 'Longitude': -86.258278000000004},\"IL\":{'Latitude': 40.349457000000001, 'Longitude': -88.986136999999999}, \n",
    " \"IN\": {'Latitude': 39.849426000000001, 'Longitude': -86.258278000000004},\"IA\":{'Latitude': 42.011538999999999, 'Longitude': -93.210526000000002},\n",
    " \"KS\": {'Latitude': 38.526600000000002, 'Longitude': -96.726485999999994},\"KY\":{'Latitude': 37.668140000000001, 'Longitude': -84.670067000000003},\n",
    " \"LA\": {'Latitude': 31.169546, 'Longitude': -91.867805000000004},\"ME\":{'Latitude': 44.693946999999994, 'Longitude': -69.381927000000005}, \n",
    " \"MD\": {'Latitude': 39.063946000000001, 'Longitude': -76.802101000000008},\"MA\":{'Latitude': 42.230170999999999, 'Longitude': -71.530106000000004},\n",
    " \"MI\": {'Latitude': 43.326617999999996, 'Longitude': -84.536094999999989},\"MN\":{'Latitude': 45.694454, 'Longitude': -93.900192000000004},\n",
    " \"MS\": {'Latitude': 32.741646000000003, 'Longitude': -89.678696000000002},\"MO\":{'Latitude': 38.456084999999995, 'Longitude': -92.288368000000006},\n",
    " \"MT\": {'Latitude': 46.921925000000002, 'Longitude': -110.454353},\"NE\":{'Latitude': 41.125370000000004, 'Longitude': -98.268081999999993},\n",
    " \"NV\": {'Latitude': 38.313515000000002, 'Longitude': -117.055374},\"NH\":{'Latitude': 43.452491999999999, 'Longitude': -71.563896},\n",
    " \"NJ\": {'Latitude': 40.298904, 'Longitude': -74.521011000000001},\"NM\":{'Latitude': 34.840515000000003, 'Longitude': -106.24848200000001},\n",
    " \"NY\": {'Latitude': 42.165725999999999, 'Longitude': -74.948051000000007},\"NC\":{'Latitude': 35.630065999999999, 'Longitude': -79.806418999999991},\n",
    " \"ND\": {'Latitude': 47.528911999999998, 'Longitude': -99.784012000000004},\"OH\":{'Latitude': 40.388783000000004, 'Longitude': -82.764915000000002},\n",
    " \"OK\": {'Latitude': 35.565342000000001, 'Longitude': -96.928916999999998},\"OR\":{'Latitude': 44.572020999999999, 'Longitude': -122.07093799999998},\n",
    " \"PA\": {'Latitude': 40.590752000000002, 'Longitude': -77.209755000000001},\"RI\":{'Latitude': 41.680892999999998, 'Longitude': -71.511780000000002},\n",
    " \"SC\": {'Latitude': 33.856891999999995, 'Longitude': -80.94500699999999},\"SD\":{'Latitude': 44.299782, 'Longitude': -99.438828000000001},\n",
    " \"TN\": {'Latitude': 35.747844999999998, 'Longitude': -86.692344999999989},\"TX\":{'Latitude': 31.054486999999998, 'Longitude': -97.563461000000004},\n",
    " \"UT\": {'Latitude': 40.150032000000003, 'Longitude': -111.86243400000001}, \"VT\":{'Latitude': 44.045876, 'Longitude': -72.710685999999995},\n",
    " \"VA\": {'Latitude': 37.769337, 'Longitude': -78.169968000000011},\"WA\":{'Latitude': 47.400902000000002, 'Longitude': -121.490494},\n",
    " \"WV\": {'Latitude': 38.491225999999997, 'Longitude': -80.954453000000001},\"WI\":{'Latitude': 44.268543000000001, 'Longitude': -89.616507999999996},\n",
    " \"WY\": {'Latitude': 42.755966000000001, 'Longitude': -107.30248999999999}}\n",
    "\n",
    "state_coords.update(abbr)\n",
    "state_names = list(state_coords.keys())\n",
    "\n",
    "locations = tweets['Geo'].tolist()\n",
    "cleaned_locations = []\n",
    "\n",
    "#Number of tweets from each coordinate\n",
    "coord_counts = {}\n",
    "\n",
    "#print (locations[:500])\n",
    "\n",
    "for location in locations:\n",
    "    \n",
    "    found_state = False\n",
    "    if type(location) is str: #make sure its not nan\n",
    "        for state in state_names:\n",
    "            if state in location:\n",
    "                coords = (state_coords[state]['Latitude'], state_coords[state]['Longitude'])\n",
    "                found_state = True\n",
    "                if coords in coord_counts:\n",
    "                    coord_counts[coords] += 1\n",
    "                else:\n",
    "                    coord_counts[coords] = 1\n",
    "                    \n",
    "                break\n",
    "    if not found_state: #tweet does not have a state name in it\n",
    "        coords = 'None'\n",
    "    cleaned_locations.append(coords)\n",
    "\n",
    "    \n",
    "name_counts = {}\n",
    "\n",
    "# Get dict with {state: number of tweets}\n",
    "for tup in list(coord_counts.keys()):\n",
    "    for key in state_coords:\n",
    "        if tup == (state_coords[key]['Latitude'], state_coords[key]['Longitude']):\n",
    "            name_counts[key] = coord_counts[tup]\n",
    "            break\n",
    "                     \n",
    "\n",
    "df = pd.DataFrame({\"Location\": cleaned_locations})\n",
    "\n",
    "tweets = pd.concat([tweets, df], axis=1)\n",
    "\n",
    "# Tweets have the fields Text, Hashtags, and Location\n",
    "del tweets['Geo']\n",
    "del tweets['Username']\n",
    "del tweets['Date']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "Clustering hashtags. Still in beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample sequence X is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-812ec27d897e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#make vectors with location and hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_with_top_hashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jessicamcaloon/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0malways\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jessicamcaloon/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample sequence X is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample sequence X is empty."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "hashtags = tweets['Hashtags'].tolist()\n",
    "\n",
    "# ignore hashtags that are redundant or don't particularly mean anything\n",
    "blacklist = [\"#womensmarch\", \"#\", \"#theresistance\", \"#theresistence\", \"#resistance\", \"#women\"]\n",
    "new_hashtags = []\n",
    "for string in hashtags:\n",
    "    string = string.lower()\n",
    "    lst = string.split()\n",
    "    for h in lst:\n",
    "        if h not in blacklist:\n",
    "            new_hashtags.append(h)\n",
    "hashtags = new_hashtags\n",
    "\n",
    "hashtags_ct = Counter(hashtags)\n",
    "top_hashtags= [x[0] for x in hashtags_ct.most_common(20)]\n",
    "#print (top_hashtags)\n",
    "\n",
    "tweets_dict = tweets.to_dict('records')\n",
    "\n",
    "# only keep tweets with top hashtags\n",
    "# create dictionary with format {tweet id: {location, [top_hashtags]} }\n",
    "tweets_with_top_hashtags = []\n",
    "\n",
    "for t in tweets_dict:\n",
    "    if t['Location'] != 'None':\n",
    "        t['Hashtags'] = t['Hashtags'].lower().split()\n",
    "        t['Hashtags'] = [h for h in t['Hashtags'] if h in top_hashtags]\n",
    "        if len (t['Hashtags']) > 0:\n",
    "            tweets_with_top_hashtags.append(t)\n",
    "        \n",
    "\n",
    "#prepare for vectorization\n",
    "for t in tweets_with_top_hashtags:\n",
    "    for h in t['Hashtags']:\n",
    "        t[h] = True\n",
    "    del t['Hashtags']\n",
    "    del t['Text']\n",
    "\n",
    "\n",
    "print (tweets_with_top_hashtags)\n",
    "#make vectors with location and hashtags\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(tweets_with_top_hashtags).toarray()\n",
    "\n",
    "\n",
    "#if we use longitude and latitude, we would scale them here\n",
    "\n",
    "#Evaluate number clusters for kmeans via silhouette score\n",
    "max_clusters = 20\n",
    "\n",
    "def sc_evaluate_clusters(X,max_clusters):\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    s[0] = 0;\n",
    "    s[1] = 0;\n",
    "    for k in range(2,max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        s[k] = metrics.silhouette_score(X,kmeans.labels_,metric='euclidean')\n",
    "    plt.plot(range(2,len(s)),s[2:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(\"K-means silhouette scores\")\n",
    "    plt.show()\n",
    "    \n",
    "sc_evaluate_clusters(X,max_clusters)\n",
    "\n",
    "k=15\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "kmeans.fit_predict(X)\n",
    "k_time = time.time() - start_time\n",
    "\n",
    "# kmeans top terms per cluster \n",
    "asc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\n",
    "order_centroids = asc_order_centroids[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#cluster label is most popular term in each cluster\n",
    "print(\"Kmeans top term per cluster:\")\n",
    "k_clusters = set()\n",
    "for i in range(k):\n",
    "    for ind in order_centroids[i, :2]:\n",
    "        k_clusters.add(terms[ind])\n",
    "print(k_clusters)\n",
    "print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "With the data from https://statusofwomendata.org, we hope to determine if there's any correlation between a state's ranking in different women's issues and the number of #WomensMarch tweets were sent from that state. \n",
    "\n",
    "The outputted data can be found in state_rankings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame as dataF\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://statusofwomendata.org/state-data/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "stateNames = []\n",
    "employmentEarn = []\n",
    "politicalPart = []\n",
    "povertyOpp = []\n",
    "reprodRights = []\n",
    "healthWell = []\n",
    "workFam = []\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "time.sleep(0.5)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "statesList = soup.find(\"article\", class_=\"post\").find_all(\"p\")\n",
    "\n",
    "for state in statesList:\n",
    "    statePage = requests.get(state.find(\"a\").get(\"href\"), headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    stateSoup = BeautifulSoup(statePage.text, \"lxml\")\n",
    "    name = stateSoup.find(\"div\", class_=\"state\").find(\"h1\").text\n",
    "    stateNames.append(name)\n",
    "    \n",
    "    reportCard = stateSoup.find(\"div\", class_=\"cardSummary\").find_all(\"tr\")[1:]\n",
    "    employmentEarn.append(reportCard[0].find_all(\"td\")[1].text)\n",
    "    if(reportCard[1].find_all(\"td\")[1].text == \"-\"):\n",
    "        politicalPart.append(\"0\")\n",
    "    else:\n",
    "        politicalPart.append(reportCard[1].find_all(\"td\")[1].text)\n",
    "    povertyOpp.append(reportCard[2].find_all(\"td\")[1].text)\n",
    "    reprodRights.append(reportCard[3].find_all(\"td\")[1].text)\n",
    "    healthWell.append(reportCard[4].find_all(\"td\")[1].text)\n",
    "    workFam.append(reportCard[5].find_all(\"td\")[1].text)\n",
    "        \n",
    "stateRankings = dataF({\"name\": stateNames, \"Employment & Earnings\": employmentEarn, \"Political Participation\": politicalPart, \"Poverty & Opportunity\": povertyOpp, \"Reproductive Rights\": reprodRights, \"Health & Well-Being\": healthWell, \"Work & Family\": workFam})\n",
    "stateRankings.to_csv(\"state_rankings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
